{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f47bc20f",
   "metadata": {},
   "source": [
    "# Extracting News from Financial Websites\n",
    "\n",
    "To extract the news, a Google browser search was performed with the following format:\n",
    "\n",
    "*   *PETR4 site:infomoney.com.br after:2020-01-01 before:2020-12-31* (Year 2020)\n",
    "\n",
    "This allowed the search results to show only the news from the specified newspaper and also within the date limits. It was very helpful in selecting the news accurately.\n",
    "\n",
    "Some websites offer a related news section that was also very important in finding more relevant news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c8c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries needed to extract the news\n",
    "# they have already been installed in the virtual environment\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import dateparser\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium_stealth import stealth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a980d560",
   "metadata": {},
   "source": [
    "## InfoMoney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1809a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of urls of the news to be extracted (InfoMoney website)\n",
    "urls_infomoney = []\n",
    "\n",
    "# creating a function to extract the news information from the infomoney website (title, date, content, newspaper)\n",
    "def extrair_dados_infomoney(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to access {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extracting the news title\n",
    "    titulo = soup.find(\"h1\")\n",
    "    titulo = titulo.get_text(strip=True) if titulo else \"Title not found\"\n",
    "\n",
    "    # Extracting the date\n",
    "    data = soup.find(\"time\")\n",
    "    data = data.get_text(strip=True) if data else \"Date not found\"\n",
    "\n",
    "    # Main content (paragraphs within the article)\n",
    "    conteudo_div = soup.find(\"article\", attrs={\"data-ds-component\": \"article\"})\n",
    "    if conteudo_div:\n",
    "        paragrafos = conteudo_div.find_all(\"p\")\n",
    "        conteudo = \"\\n\".join(p.get_text(\" \", strip=True) for p in paragrafos)\n",
    "    else:\n",
    "        conteudo = \"Content not found\"\n",
    "\n",
    "    # The return will be a dictionary containing all the important fields\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"titulo\": titulo,\n",
    "        \"data\": data,\n",
    "        \"conteudo\": conteudo,\n",
    "        \"site\": \"Infomoney\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f485718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an empty list that will store all news from the url list\n",
    "for url in urls_infomoney:\n",
    "    dados_noticia = extrair_dados_infomoney(url)\n",
    "    if dados_noticia:\n",
    "        updated_date = dateparser.parse(dados_noticia[\"data\"], languages=[\"pt\"])\n",
    "        dados_noticia[\"data\"] = updated_date.strftime(\"%d/%m/%Y %H:%M\")\n",
    "        noticias.append(dados_noticia)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24481476",
   "metadata": {},
   "source": [
    "## MoneyTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be372c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_moneytimes = []\n",
    "\n",
    "def extrair_dados_moneytimes(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to access {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extracting the news title\n",
    "    titulo = soup.find(\"h1\")\n",
    "    titulo = titulo.get_text(strip=True) if titulo else \"Title not found\"\n",
    "\n",
    "    # Extracting the date\n",
    "    data = soup.find(\"span\", attrs={\"class\": \"single_meta_author_infos_date_time\"})\n",
    "    data = data.get_text(strip=True) if data else \"Date not found\"\n",
    "\n",
    "    # Main content (paragraphs within the article)\n",
    "    conteudo_div = soup.find(\"div\", attrs={\"class\": \"single_block_news_text\"})\n",
    "    if conteudo_div:\n",
    "        paragrafos = conteudo_div.find_all(\"p\")\n",
    "        conteudo = \"\\n\".join(p.get_text(\" \", strip=True) for p in paragrafos)\n",
    "    else:\n",
    "        conteudo = \"Content not found\"\n",
    "\n",
    "    # The return will be a dictionary containing all the important fields\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"titulo\": titulo,\n",
    "        \"data\": data,\n",
    "        \"conteudo\": conteudo,\n",
    "        \"site\": \"Money Times\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f928a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The date for this website is in the following example format: 22 jul 2020, 15:19 (I need to format it)\n",
    "noticias = []\n",
    "for url in urls_moneytimes:\n",
    "    dados_noticia = extrair_dados_moneytimes(url)\n",
    "    if dados_noticia:\n",
    "        updated_date = dateparser.parse(dados_noticia[\"data\"], languages=[\"pt\"])\n",
    "        dados_noticia[\"data\"] = updated_date.strftime(\"%d/%m/%Y %H:%M\")\n",
    "        noticias.append(dados_noticia)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc6f42d",
   "metadata": {},
   "source": [
    "## Seu Dinheiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "643d3bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://www.suno.com.br/noticias/petrobras-petr4-define-dividendos-3t24-gss/', 'titulo': 'Petrobras (PETR4) define como pagará dividendos do 3T', 'data': '11/12/2024 11:35', 'conteudo': 'A Petrobras ( PETR4 ) anunciou detalhes sobre a distribuição de R$ 17,12 bilhões em proventos aos acionistas, referentes aos resultados do terceiro trimestre de 2024.\\n\\nA decisão, aprovada pelo Conselho de Administração da empresa em 7 de novembro, inclui pagamentos em duas parcelas, programados para 2025, e ajustes baseados na taxa Selic.\\nAlém, foi confirmado o pagamento de dividendos extraordinários da Petrobras no valor R$ 20 bilhões ainda este ano.\\nOs R$ 17,12 bilhões em dividendos da Petrobras , equivalentes a R$ 1,32 por ação ordinária e preferencial, serão pagos da seguinte forma:\\nAmbas as parcelas terão seus valores corrigidos pela variação da taxa Selic entre 31 de dezembro de 2024 e as datas de pagamento.\\nOs pagamentos sob a forma de JCP da Petrobras estão sujeitos à retenção de 15% de Imposto de Renda , reduzindo o valor líquido para acionistas sem isenção ou imunidade tributária.\\nOs investidores com ações da Petrobras até 23 de dezembro de 2024 terão direito aos proventos. As ações passarão a ser negociadas ex- dividendos na B3 a partir de 26 de dezembro, e para os ADRs (recibos de ações negociados em Nova York), a data de corte é 27 de dezembro.\\nAdicionalmente, a Petrobras pagará R$ 20 bilhões em dividendos extraordinários , equivalentes a R$ 1,55 por ação, conforme anunciado em 21 de novembro.\\nO pagamento será efetuado em 23 de dezembro de 2024 aos acionistas que estiverem posicionados na Petrobras até 11 de dezembro. Para as ADRs, a data de corte é em 13 de dezembro.\\n', 'site': 'Suno Notícias'}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"noticias_petrobras_2024.csv\")\n",
    "\n",
    "noticias = df.to_dict(orient=\"records\")\n",
    "\n",
    "print(noticias[len(noticias)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db39fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The normal code, only with bs4, was giving response code 202. That is, the server received the request and accepted to process it,\n",
    "# but processing has not yet been completed. In the other scripts the response code is 200. That is, the request has already been successfully processed and sent.\n",
    "\n",
    "urls_seudinheiro = []\n",
    "\n",
    "def extrair_dados_seudinheiro(url):\n",
    "    # Selenium settings (optional headless mode)\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # remove if you want to see the browser\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # wait for JS loading\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Title\n",
    "        titulo = soup.find(\"h1\")\n",
    "        titulo = titulo.get_text(strip=True) if titulo else \"Title not found\"\n",
    "\n",
    "        # Date\n",
    "        data = soup.select_one(\"div.js-first-letter.single__date-time\")\n",
    "        data = data.get_text(strip=True) if data else \"Date not found\"\n",
    "\n",
    "        # Main content\n",
    "        conteudo_div = soup.find(\"div\", class_=[\"newSingle_content_right\", \"default\"], id=\"js-first-letter\")\n",
    "        if conteudo_div:\n",
    "            paragrafos = conteudo_div.find_all(\"p\")\n",
    "            conteudo = \"\\n\".join(p.get_text(\" \", strip=True) for p in paragrafos)\n",
    "        else:\n",
    "            conteudo = \"Content not found\"\n",
    "\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"titulo\": titulo,\n",
    "            \"data\": data,\n",
    "            \"conteudo\": conteudo,\n",
    "            \"site\": \"Seu Dinheiro\"\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to access {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c37ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incrementing the news list\n",
    "# The date for this website is in the following example format: 5 de março de 202013:25 (I need to format it)\n",
    "\n",
    "for url in urls_seudinheiro:\n",
    "    dados_noticia = extrair_dados_seudinheiro(url)\n",
    "    if dados_noticia:\n",
    "        data_str = dados_noticia[\"data\"]\n",
    "\n",
    "        # 1. Remove tabs and anything after \" - atualizado\"\n",
    "        data_str = re.sub(r\"\\s+- atualizado.*\", \"\", data_str).strip()\n",
    "\n",
    "        # 2. Fix space between year and time (202012:34 → 2020 12:34)\n",
    "        data_str = re.sub(r\"(\\d{4})(\\d{2}:\\d{2})\", r\"\\1 \\2\", data_str)\n",
    "        updated_date = dateparser.parse(data_str, languages=[\"pt\"])\n",
    "\n",
    "        if updated_date:  \n",
    "            dados_noticia[\"data\"] = updated_date.strftime(\"%d/%m/%Y %H:%M\")\n",
    "        else:\n",
    "            dados_noticia[\"data\"] = data_str\n",
    "\n",
    "        noticias.append(dados_noticia)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8da63c",
   "metadata": {},
   "source": [
    "## Suno News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_suno = []\n",
    "\n",
    "def extrair_dados_suno(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to access {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extracting the news title\n",
    "    titulo = soup.find(\"h1\")\n",
    "    titulo = titulo.get_text(strip=True) if titulo else \"Title not found\"\n",
    "\n",
    "    # Extracting the date\n",
    "    data = soup.find(\"time\", attrs={\"itemprop\" : \"datePublished\"})\n",
    "    data = data.get_text(strip=True) if data else \"Date not found\"\n",
    "\n",
    "    # Main content (paragraphs within the article)\n",
    "    conteudo_div = soup.find(\"article\")\n",
    "    if conteudo_div:\n",
    "        paragrafos = conteudo_div.find_all(\"p\")\n",
    "        conteudo = \"\\n\".join(p.get_text(\" \", strip=True) for p in paragrafos)\n",
    "    else:\n",
    "        conteudo = \"Content not found\"\n",
    "\n",
    "    # The return will be a dictionary containing all the important fields\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"titulo\": titulo,\n",
    "        \"data\": data,\n",
    "        \"conteudo\": conteudo,\n",
    "        \"site\": \"Suno Notícias\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3d8a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incrementing the news list\n",
    "for url in urls_suno:\n",
    "    dados_noticia = extrair_dados_suno(url)\n",
    "    if dados_noticia:\n",
    "        noticias.append(dados_noticia)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989b6c2",
   "metadata": {},
   "source": [
    "## Uol Economy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1903ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://economia.uol.com.br/financas-pessoais/noticias/redacao/2020/03/25/bolsa-acao-petrobras-queda-o-que-fazer-vender-comprar-manter.htm\n",
    "\n",
    "urls_uol = []\n",
    "\n",
    "def extrair_dados_uol(url):\n",
    "    # Selenium settings\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--user-data-dir=/tmp/chrome_user_data\") # Add this line\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/113.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    # Applying \"stealth\" to mask that it's Selenium\n",
    "    stealth(\n",
    "        driver,\n",
    "        languages=[\"pt-BR\", \"pt\", \"en-US\", \"en\"],\n",
    "        vendor=\"Google Inc.\",\n",
    "        platform=\"Win32\",\n",
    "        webgl_vendor=\"Intel Inc.\",\n",
    "        renderer=\"Intel Iris OpenGL Engine\",\n",
    "        fix_hairline=True,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # wait for JS loading\n",
    "\n",
    "        # Get the HTML rendered by the browser\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # Extracting the news title\n",
    "        titulo = soup.find(\"h1\")\n",
    "        titulo = titulo.get_text(strip=True) if titulo else \"Title not found\"\n",
    "\n",
    "        # Extracting the date\n",
    "        data = soup.find(\"p\", class_=[\"time\"])\n",
    "        data = data.get_text(strip=True) if data else \"Date not found\"\n",
    "\n",
    "        # Main content (paragraphs within the article)\n",
    "        conteudo_div = soup.find(\"div\", class_=[\"text\"])\n",
    "        if conteudo_div:\n",
    "            paragrafos = conteudo_div.find_all(\"p\")\n",
    "            conteudo = \"\\n\".join(p.get_text(\" \", strip=True) for p in paragrafos)\n",
    "        else:\n",
    "            conteudo = \"Content not found\"\n",
    "\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"titulo\": titulo,\n",
    "            \"data\": data,\n",
    "            \"conteudo\": conteudo,\n",
    "            \"site\": \"Uol Economia\"\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to access {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e1ce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incrementing the news list\n",
    "for url in urls_uol:\n",
    "    dados_noticia = extrair_dados_uol(url)\n",
    "    if dados_noticia:\n",
    "        noticias.append(dados_noticia)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa292d",
   "metadata": {},
   "source": [
    "## Saving the news to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c4703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the list to a DataFrame\n",
    "df = pd.DataFrame(noticias)\n",
    "# Saving the DataFrame to a CSV file\n",
    "df.to_csv(\"noticias_petrobras_2024.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54961985",
   "metadata": {},
   "source": [
    "## Counting news by website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463230bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site\n",
      "Suno Notícias    270\n",
      "Money Times      238\n",
      "Seu Dinheiro     177\n",
      "Uol Economia      25\n",
      "Infomoney         10\n",
      "Name: count, dtype: int64\n",
      "Total de notícias: 720\n",
      "site\n",
      "Suno Notícias    304\n",
      "Money Times      220\n",
      "Seu Dinheiro     198\n",
      "Infomoney         24\n",
      "Uol Economia       6\n",
      "Name: count, dtype: int64\n",
      "Total de notícias: 752\n",
      "site\n",
      "Suno Notícias    292\n",
      "Money Times      221\n",
      "Seu Dinheiro     113\n",
      "Infomoney         42\n",
      "Uol Economia      22\n",
      "Name: count, dtype: int64\n",
      "Total de notícias: 690\n",
      "site\n",
      "Suno Notícias    385\n",
      "Money Times      165\n",
      "Seu Dinheiro     104\n",
      "Infomoney         93\n",
      "Name: count, dtype: int64\n",
      "Total de notícias: 747\n"
     ]
    }
   ],
   "source": [
    "def imprimir_quantidade_noticias(nome_arquivo_csv: str):   \n",
    "    # Loading the CSV file\n",
    "    df = pd.read_csv(nome_arquivo_csv)\n",
    "    # Counting the number of news per website\n",
    "    contagem_sites = df['site'].value_counts()\n",
    "    # printing the news count per website\n",
    "    print(contagem_sites)\n",
    "    # printing the total news\n",
    "    print(f\"Total news: {len(df)}\")\n",
    "\n",
    "# imprimir_quantidade_noticias(\"noticias_final_2020.csv\")\n",
    "# imprimir_quantidade_noticias(\"noticias_final_2021.csv\")\n",
    "# imprimir_quantidade_noticias(\"noticias_final_2023.csv\")\n",
    "# imprimir_quantidade_noticias(\"noticias_final_2024.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
